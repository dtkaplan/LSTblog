[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The beta version of Lessons in Statistical Thinking (LST) was released in February 2024.\nLST is intended as a first statistics course but many of the topics are suitable for a second-statistics course as well. There are many innovations in LST. This blog has posts to orient new instructors both to the topics in LST and the software provided by the {LSTbook} R package.\n\nBanner image: Detail from Paul Signac’s La Corne d’Or (1907)"
  },
  {
    "objectID": "posts/WebR/index.html",
    "href": "posts/WebR/index.html",
    "title": "LST computing in your browser",
    "section": "",
    "text": "This page provides an easy way to get started with computing for Lessons in Statistical Thinking.\nInstructions:\n\nWrite your commands in the code chunk.\nSometimes it takes about 15 seconds (or longer, depending on your web connection)for this page to initialize. (You’ll see a “installing package” message in the chunk during initialization.) Keeping this page open in your browser will help speed the initialization.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWrite your R statements here.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe following chunks are for your convenience. For instance, you might want to copy-and-paste some code from the previous chunk in order to be able to refer to it later.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStudent Name/Section/etc.\n\n\nDataPoint plotsWranglingModel shapesSimulationWranglingComputingComputing\n\n\n\n\n\n\n\n\nData frames\n\n\n\n\n\nMany data frames are already available by name. Commonly used examples in Lessons include: Birdkeepers, Grades, Galton, Hill_racing, Penguins, SAT, TenMileRace, Whickham\nLooking at the codebook for a data frame\nUse ? followed by the name of the data frame.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSee the first several rows\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSee the variable names\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\n\nPipeline to point_plot()\n\n\n\n\n\nThe most common operation in Lessons is to make a point plot of a response variable versus one or more explanatory variables. To illustrate, here is a point plot of flipper length versus species for three different species of penguins, using the data recorded in the Penguins data frame.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe name of the data frame comes first in the R command. The |&gt; creates a pipeline into an operation on the data. The particular operation here is point_plot().\npoint_plot() requires one argument: a tilde expression with the name of the response variable on the left and the explanatory variable on the right. In the above example, the tilde expression is flipper ~ species. (The squiggly character, \\({\\LARGE \\sim}\\), is called a “tilde.”)\nActivity:\n\nEach dot in the point plot corresponds to a single row in the Penguins data frame. The dots are slightly spread apart horizontally at random. What is this spread-apart technique called and why is it useful.\nAdd a violin annotation to the above plot by giving a second argument, annot = \"violin\". (Note the quotes around \"violin\".) For what value of flipper is the Adelie species violin the thickest? Choose one of 175mm, 185mm, 190mm, 200mm. How about for the Chinstrap penguins?\nRemove the violin annotation and replace it with annot = \"model\". What do the annotations look like?\nEach species has its own model annotation. The vertical extent of the annotation indicates the precision of the model. When two such annotations (for different species) overlap vertically, the statistical methodology is telling us that we cannot statistically discern that the model values are different for the two species. Do any of the annotations overlap vertically?\n\n\nNote: We know that flipper is in mm from the documentation (also sometimes called the “codebook”) for the palmerpenguins::penguins data frame. To see the codebook, use ? followed by the name of the data frame. Try it!\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStorage arrow\n\n\n\n\n\nThe following chunk trains a simple model of the child’s height as a function of the heights of the mother and father. Using the storage arrow (&lt;-), the trained model is stored under the name Mod1.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHaving stored the trained model, you can access it to look at it or perform additional operations on it in a subsequent chunk. Of course, you have to “Run Code” the first chunk before you can use those results in any subsequent chunk.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nDependent code chunks\n\n\n\n\n\nThe following chunk trains a simple model of the child’s height as a function of the heights of the mother and father. Using the storage arrow (&lt;-) the model is stored under the name Mod1\nChunk 1\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nChunk 2, below, does some additional processing on the Mod1 object.\nChunk 2\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nPress “Run Code” in Chunk 1 and then in Chunk 2. So long as you run Chunk 1 first, Chunk 2 will have access to the already created Mod1. But if you try to run Chunk 2 before having run Chunk 1, you will get an error message.\nActivity\n\nPress the “start over” icon () in Chunks 1 and 2 to reset the chunks to their original content.\nRun Code Block 2 without having previously run Block 1. What happens?\nNow run Block 1. Afterwards, run Block 2. What has changed from (a)?\nPress  in Block 1. then run Block 2 again. What happens?\nRun Block 1 again, then press  for Block 2. Can you then run Block 2 successfully?\n\n\n\n\n\n\n\nAnnotated point plots\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "posts/Lesson-12-Adjustment/index.html",
    "href": "posts/Lesson-12-Adjustment/index.html",
    "title": "Lesson 12 Resources",
    "section": "",
    "text": "To illustrate how covariates set context, consider an issue of interest to public policy-makers in many societies: How much money to spend on children’s education? State lawmakers in the US are understandably concerned with the quality of public education provided. However, they also have other concerns and constraints and constituencies who give budget priority to other matters.\nIn evaluating their various trade-offs, lawmakers could benefit by knowing how increased educational spending will shape educational outcomes. What can available data tell us? Unfortunately, there are various political constraints that work against states adopting and publishing data on a standard, genuine measure of educational outcome. Instead, we have high-school graduation rates, student grades, and other non-standardized data. These data might have some meaning but can also reflect system gaming by administrators and teachers.\nAlthough imperfect, college admissions tests such as the ACT and SAT provide consistent data between states. For example, Figure 1 shows the average SAT score in 2010 in each state versus expenditures per pupil in public elementary and secondary schools. Layered on top of the data is a flexible linear model (and its confidence band) of SAT score versus expenditure.\nThe overall impression given by the model is that the relationship is negative, with lower expenditures corresponding to higher SAT scores. However, the confidence band is broad; it is possible to find a smooth path with almost zero slope through the confidence band. Either way, this graph does not support the conventional wisdom that higher spending produces better school outcomes.\n\n\nCode\ndata(SAT_2010, package = \"mdsr\")\nmod_1 &lt;- SAT_2010 |&gt; model_train(total ~ expenditure)\nmodel_plot(mod_1, interval = \"confidence\") |&gt;\n  gf_point(total ~ expenditure,  point_ink = 0.5, data = SAT_2010, inherit=FALSE) |&gt;\n  gf_labs(y = \"Average SAT score\", x = \"Public school expenditures per pupil ($1000s)\")\n\n\n\n\n\n\n\n\nFigure 1: State by state data (from 2010) on average SAT college admissions test scores and expenditures for public education.\n\n\n\n\n\nOf course, other factors play a role in shaping education outcomes: for instance, poverty levels, parental education, and how the educational money is spent (higher pay for teachers or smaller class sizes? administrative bloat?).\nAt first glance, it is tempting to ignore these additional factors, particularly if we do not have data on them. Moreover, as our interest is in understanding the relationship between expenditures and education outcomes, we are not directly concerned with the additional factors. However, the lack of direct concern does not imply that we should ignore the factors but that we should do what we can to “hold them constant”.\nTo illustrate, consider the fraction of eligible students (those in their last year of high school) who take the college admission test. This fraction varies widely from state to state. In a poor state where few students go to college, the fraction can be tiny (Alabama 8%, Arkansas 5%, Mississippi 4%, Louisiana 8%). In some other states, the large majority of students take the SAT (Maine 93%, Massachusetts 89%, New York 89%). In states with low SAT participation rates, the students who take the test tend to be those applying to schools with competitive admissions. Such strong students will get high scores. In contrast, the scores in states with high participation rates reflect both strong and weak students. Consequently, the scores will be lower on average than in the low-participation states.\nPutting the relationship between expenditure and SAT scores in the context of the fraction taking the SAT is accomplished with the model SAT ~ expenditure + fraction rather than just SAT ~ expenditure. Figure 2 shows a model with fraction as a covariate.\n\n\nCode\nmod &lt;- SAT_2010 |&gt; model_train(total ~ expenditure * sat_pct)\nmodel_plot(mod, interval = \"confidence\") |&gt;\n  gf_labs(y = \"Average SAT score\", x = \"Public school expenditures per pupil ($1000s)\")\n\n\n\n\n\n\n\n\nFigure 2: The model of SAT score versus expenditures, including as a covariate the fraction of eligible students in the state who take the SAT.\n\n\n\n\n\nNote that the effect size of spending on SAT scores is positive when the expenditure level is less than $10,000 per pupil. Notice as well that when the fraction taking the SAT is tiny, the average scores do not depend on expenditure. This flat relationship suggests that, among elite students, state expenditure does not make a discernible difference. Perhaps the college-bound students in such states have other educational resources to draw on.\nThe relationship shown in Figure 1 is genuine. However, so is the different relationship seen in Figure 2. How can the same data be consistent with two utterly different displays? The answer, perhaps unexpectedly, has to do with the connections among the explanatory variables. Whatever the relationship between an individual explanatory variable and the response variable, the appearance of that relationship will depend on which covariates the modeler chooses to include."
  },
  {
    "objectID": "posts/Lesson-12-Adjustment/index.html#example-spending-and-student-performance",
    "href": "posts/Lesson-12-Adjustment/index.html#example-spending-and-student-performance",
    "title": "Lesson 12 Resources",
    "section": "",
    "text": "To illustrate how covariates set context, consider an issue of interest to public policy-makers in many societies: How much money to spend on children’s education? State lawmakers in the US are understandably concerned with the quality of public education provided. However, they also have other concerns and constraints and constituencies who give budget priority to other matters.\nIn evaluating their various trade-offs, lawmakers could benefit by knowing how increased educational spending will shape educational outcomes. What can available data tell us? Unfortunately, there are various political constraints that work against states adopting and publishing data on a standard, genuine measure of educational outcome. Instead, we have high-school graduation rates, student grades, and other non-standardized data. These data might have some meaning but can also reflect system gaming by administrators and teachers.\nAlthough imperfect, college admissions tests such as the ACT and SAT provide consistent data between states. For example, Figure 1 shows the average SAT score in 2010 in each state versus expenditures per pupil in public elementary and secondary schools. Layered on top of the data is a flexible linear model (and its confidence band) of SAT score versus expenditure.\nThe overall impression given by the model is that the relationship is negative, with lower expenditures corresponding to higher SAT scores. However, the confidence band is broad; it is possible to find a smooth path with almost zero slope through the confidence band. Either way, this graph does not support the conventional wisdom that higher spending produces better school outcomes.\n\n\nCode\ndata(SAT_2010, package = \"mdsr\")\nmod_1 &lt;- SAT_2010 |&gt; model_train(total ~ expenditure)\nmodel_plot(mod_1, interval = \"confidence\") |&gt;\n  gf_point(total ~ expenditure,  point_ink = 0.5, data = SAT_2010, inherit=FALSE) |&gt;\n  gf_labs(y = \"Average SAT score\", x = \"Public school expenditures per pupil ($1000s)\")\n\n\n\n\n\n\n\n\nFigure 1: State by state data (from 2010) on average SAT college admissions test scores and expenditures for public education.\n\n\n\n\n\nOf course, other factors play a role in shaping education outcomes: for instance, poverty levels, parental education, and how the educational money is spent (higher pay for teachers or smaller class sizes? administrative bloat?).\nAt first glance, it is tempting to ignore these additional factors, particularly if we do not have data on them. Moreover, as our interest is in understanding the relationship between expenditures and education outcomes, we are not directly concerned with the additional factors. However, the lack of direct concern does not imply that we should ignore the factors but that we should do what we can to “hold them constant”.\nTo illustrate, consider the fraction of eligible students (those in their last year of high school) who take the college admission test. This fraction varies widely from state to state. In a poor state where few students go to college, the fraction can be tiny (Alabama 8%, Arkansas 5%, Mississippi 4%, Louisiana 8%). In some other states, the large majority of students take the SAT (Maine 93%, Massachusetts 89%, New York 89%). In states with low SAT participation rates, the students who take the test tend to be those applying to schools with competitive admissions. Such strong students will get high scores. In contrast, the scores in states with high participation rates reflect both strong and weak students. Consequently, the scores will be lower on average than in the low-participation states.\nPutting the relationship between expenditure and SAT scores in the context of the fraction taking the SAT is accomplished with the model SAT ~ expenditure + fraction rather than just SAT ~ expenditure. Figure 2 shows a model with fraction as a covariate.\n\n\nCode\nmod &lt;- SAT_2010 |&gt; model_train(total ~ expenditure * sat_pct)\nmodel_plot(mod, interval = \"confidence\") |&gt;\n  gf_labs(y = \"Average SAT score\", x = \"Public school expenditures per pupil ($1000s)\")\n\n\n\n\n\n\n\n\nFigure 2: The model of SAT score versus expenditures, including as a covariate the fraction of eligible students in the state who take the SAT.\n\n\n\n\n\nNote that the effect size of spending on SAT scores is positive when the expenditure level is less than $10,000 per pupil. Notice as well that when the fraction taking the SAT is tiny, the average scores do not depend on expenditure. This flat relationship suggests that, among elite students, state expenditure does not make a discernible difference. Perhaps the college-bound students in such states have other educational resources to draw on.\nThe relationship shown in Figure 1 is genuine. However, so is the different relationship seen in Figure 2. How can the same data be consistent with two utterly different displays? The answer, perhaps unexpectedly, has to do with the connections among the explanatory variables. Whatever the relationship between an individual explanatory variable and the response variable, the appearance of that relationship will depend on which covariates the modeler chooses to include."
  },
  {
    "objectID": "posts/Lesson-12-Adjustment/index.html#a-medical-example",
    "href": "posts/Lesson-12-Adjustment/index.html#a-medical-example",
    "title": "Lesson 12 Resources",
    "section": "A medical example",
    "text": "A medical example\nThis news report appeared in 2007:\n\nHeart Surgery Drug Carries High Risk, Study Says. A drug widely used to prevent excessive bleeding during heart surgery appears to raise the risk of dying in the five years afterward by nearly 50 percent, an international study found. The researchers said replacing the drug—aprotinin, sold by Bayer under the brand name Trasylol—with other, cheaper drugs for a year would prevent 10,000 deaths worldwide over the next five years.\nBayer said in a statement that the findings are unreliable because Trasylol tends to be used in more complex operations, and the researchers’ statistical analysis did not fully account for the complexity of the surgery cases. The study followed 3,876 patients who had heart bypass surgery at 62 medical centers in 16 nations. Researchers compared patients who received aprotinin to patients who got other drugs or no antibleeding drugs. Over five years, 20.8 percent of the aprotinin patients died, versus 12.7 percent of the patients who received no antibleeding drug. [This is a 64% increase in the death rate.] When researchers adjusted for other factors, they found that patients who got Trasylol ran a 48 percent higher risk of dying in the five years afterward. The other drugs, both cheaper generics, did not raise the risk of death significantly.  The study was not a randomized trial, meaning that it did not randomly assign patients to get aprotinin or not. In their analysis, the researchers took into account how sick patients were before surgery, but they acknowledged that some factors they did not account for may have contributed to the extra deaths. - Carla K. Johnson, Associated Press, 7 Feb. 2007“Significant” has a specialized meaning in statistical language. It is not a synonym for “important.” See Lessons 36 through 38\n\nThe report involves several variables. Of primary interest is the relationship between (1) the risk of dying after surgery and (2) the drug used with the goalo of preventing excessive bleeding during surgery. Also potentially important are (3) the complexity of the surgical operation and (4) how sick the patients were before surgery. Bayer disputes the published results of the relationship between (1) and (2) holding (4) constant, saying that it is also essential to hold variable (3) constant.\nThe total relationship involves a death rate of 20.8 percent of patients who got aprotinin versus 12.7 percent for the patients taking the generic drugs: an increase in the death rate by a factor of 1.64. However, when the researchers looked at a partial relationship (holding constant patient sickness before the operation), the effect size of aprotinin on mortality was less: a factor of 1.48. In other words, the model death ~ aprotinin shows a 64% increase in the death rate, but the model death ~ aprotinin + sickness shows a slightly smaller increase in death rate: 48%. The difference between the two estimates reflects doctors being more likely to give aprotinin to sicker patients.\nThe story’s last paragraph states that the choice of patients receiving aprotinin versus the generic drugs was not made at random. Some readers may find this reassuring. Why in the world would anyone prescribe a drug at random? The point, however, is to select randomly who gets which drug among the patients for whom the drugs would be appropriate. The phrase “randomized trial” used in the paragraph means specifically an experiment in which one treatment or the other—aprotinin versus the generic drugs—is assigned at random. The virtues of experiment and the vital role of random assignment are detailed in Lesson 26."
  },
  {
    "objectID": "posts/Lesson-12-Adjustment/index.html#latin",
    "href": "posts/Lesson-12-Adjustment/index.html#latin",
    "title": "Lesson 12 Resources",
    "section": "Latin",
    "text": "Latin\n\nDifferent fields use their own vocabulary. For instance, in economics the phrase ceteris paribus is often used. Engaging such vocabulary in statistics helps students see the relevance to their own field of interest.*\n\nUsing covariates in models enables the relationship between a response and an explanatory variable to be described ceteris paribus, that is, “all other things being equal.” Another phrase used in news stories is “after adjusting for ….” This is appropriate since the all in “all other things” is, in reality, refers only to those particular factors used as the covariates in the model. Dr. Meyer’s foot width results might be stated in everyday language as, “After adjusting for foot length, she found no difference in the widths of girls’ and boys’ feet.”\nNot including covariates in a model amounts to “letting other things change as they will.” In Latin, this is “mutatis mutandis.” In the foot-width example, the model width ~ sex looks at the differences in foot width for the two sexes. However, sex is not the only thing associated with foot width. The model width ~ sex ignores all other factors than sex; it compares boys and girls mutatis mutandis, that is, letting other things change as they will. In this case, comparing boys and girls involves not just the possible differences in foot width but also the differences in other factors such as foot length and body weight."
  },
  {
    "objectID": "posts/Lesson-12-Adjustment/index.html#example-one-change-can-bring-another",
    "href": "posts/Lesson-12-Adjustment/index.html#example-one-change-can-bring-another",
    "title": "Lesson 12 Resources",
    "section": "Example: One change can bring another",
    "text": "Example: One change can bring another\n\nHere’s an economics/management related story about mutatis mutandis.\n\nI was once involved in a budget committee that recommended employee health benefits for the college where I worked. At the time, college employees who belonged to the college’s insurance plan received a generous subsidy for their health insurance costs. Employees who did not belong to the plan received no subsidy but were given a modest monthly cash payment. After the stock market crashed in 2000, the college needed to cut budgets. One proposal called for eliminating the cash payment to employees who did not belong to the insurance plan. Proponents of the plan claimed that this would save money without reducing health benefits. I argued that this claim was an “all other things being equal” analysis: how expenditures would change assuming the number of people belonging to the insurance plan remained constant. In reality, however, the policy change would play out mutatis matandis; the loss of the cash payment would cause some employees, who currently received health benefits through their spouse’s health plan, to switch to the college’s health plan. That is what happened, contributing to an overall increase in healthcare expenses."
  },
  {
    "objectID": "posts/Lesson-23-Confidence-intervals/index.html",
    "href": "posts/Lesson-23-Confidence-intervals/index.html",
    "title": "Lesson 23 Resources",
    "section": "",
    "text": "This is a story about the importance of including covariates to answer research questions. Confidence intervals can be “correct” without being informative if appropriate covariates are not considered.\n\nDr. Mary Meyer is a statistics professor at Colorado State University. In 2006, she published an article recounting an episode from family life:\n\nWhen my daughter was in fourth grade, I took her shopping for dress shoes. I was disappointed in the quality of girls’ shoes at every store in the mall. The shoes for boys were sturdy and had plenty of room in the toes. On the other hand, shoes for girls were flimsy, narrow, and had pointed toes. In spite of the better construction for boys, the costs of the shoes were similar! For children the same age, boys had shoes they could run around in, while girls’ shoes were clearly for style and not comfort.\n\n\nUpon complaining about this state of affairs, I was told by sales representatives in two stores that boys actually had wider feet than girls, so needed wider shoes. Being very skeptical, I thought I would test this claim.\n\nWe will return to Dr. Meyer’s project in a little bit. However, for now, imagine how this situation might be addressed by someone who has yet to develop good statistical thinking skills. We will call this imagined protagonist “Mr. Shoebuyer.” Since the salesmen claimed that girls’ feet are narrower than boys, Mr. Shoebuyer heads out to measure the widths of girls’ and boys’ shoes.\nA shoe store provides a convenient place to measure the widths of many different shoe styles. Mr. Shoebuyer gets to the shoe store, heads to the children’s section, and starts measuring. For each shoe on display, he records the shoe width and whether the shoe is for girls or boys. Here are his data:\n\n\n\n\n\nsex\nwidth\n\n\n\n\nG\n9.0\n\n\nG\n9.0\n\n\nG\n8.6\n\n\nG\n9.3\n\n\nB\n9.2\n\n\nB\n9.7\n\n\nB\n8.8\n\n\nB\n9.3\n\n\n\n\n\nOnce back home, Mr. Shoebuyer uses his calculator to find the mean width of the shoes in each group. His results surprise him:\n\n\n\nsex\nmean width\n\n\n\n\nGirls\n8.98 cm\n\n\nBoys\n9.25 cm\n\n\n\nMr. Shoebuyer happens to be your uncle. He knows you are taking a statistics course and asks you to check his arithmetic. Putting on a statistical thinking hat to the effect size of sex on shoe width, you note the absence of a confidence interval. This omission is easy to fix.\n\nShoebuyer_data |&gt; model_train(width ~ sex) |&gt; conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr  .coef  .upr\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 (Intercept)  8.84   9.25  9.66 \n2 sexG        -0.848 -0.275 0.298\n\n\nYour uncle is at the table at Thanksgiving break. “Sorry, Uncle, but you don’t have nearly enough data to conclude that girls’ feet are narrower than boys’.” Translating the confidence interval into plus-or-minus format, you point out that the difference between the sexes is \\(0.275 \\pm 0.6\\) cm. “You’ll need enough data to get that 0.6 margin of error down to something like 0.2 or lower.” You also point out that there might be a better place to collect data than a shoe store. “It’s the feet, not the shoes, that you want to measure.”\nAware of these pitfalls, Dr. Meyer worked with the third- and fourth-grade teachers at her daughter’s school to collect data. Being a statistical thinker, she thought about what data would illuminate the matter before carrying out the data collection. Her data, a sample of size \\(n=39\\), are recorded in the KidsFeet data frame.\n\nKidsFeet |&gt; model_train(width ~ sex) |&gt; conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr  .coef    .upr\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  8.98   9.19   9.40  \n2 sexG        -0.713 -0.406 -0.0990\n\n\nIn plus-or-minus format, this confidence interval is \\(-0.4 \\pm 0.3\\) cm. Whatever the format, Dr. Meyer’s data provides some evidence that girls’ feet are narrower than boys’.\nAs a statistical thinker, Dr. Meyer knows that even though the foot width is the original quantity of interest, other factors might play a role in the system. For example, boys’ feet might trend longer or shorter than girls’ feet. This possibility should be taken into account by looking at the effect size of sex on width, holding length constant. After all, a shoe buyer first tells the salesperson their foot length (or “size”); the salesperson then brings shoes of that size to try on.\n\nKidsFeet |&gt; model_train(width ~ sex + length) |&gt; conf_interval()\n\n# A tibble: 3 × 4\n  term          .lwr  .coef   .upr\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept)  1.10   3.64  6.18  \n2 sexG        -0.495 -0.233 0.0297\n3 length       0.120  0.221 0.322 \n\n\nAlthough sex is the explanatory variable of primary interest to Dr. Meyer’s question, she knows to include other explanatory variables that might play a role. Such explanatory variables, not of direct interest, are called “covariates.” Dr. Meyer’s statistical expertise led her to consider possible covariates before collecting her data and took the trouble of measuring both foot length and width.\nThe confidence interval on the sexG coefficient includes zero when length is taken into account. Dr. Meyer’s little study provides evidence that even if girls’ shoes tend to be narrower than boys’, the feet inside them have about the same shape for both sexes.\n\n\nSometimes the variables originally collected can be used to formula a new variable that better addresses the research question. In this story, the issue is the shape of shoes. This involves at least two variables: width and length. In the previous narrative we used a general-purpose procedure—adjustment for covariates—to enable us to focus on foot width while taking length into account.\nAn alternative approach is to calculate a new variable that describes both the width and length aspects of foot shape: aspect ratio.\n\nKidsFeet |&gt; \n  mutate(aspect_ratio = width / length) |&gt; \n  model_train(aspect_ratio ~ sex) |&gt; \n  conf_interval()\n\n# A tibble: 2 × 4\n  term           .lwr    .coef    .upr\n  &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  0.359   0.366   0.374  \n2 sexG        -0.0160 -0.00485 0.00631\n\n\nWhen using aspect_ratio, the model need not include length as a covariate; the aspect ratio itself “adjusts” width for length."
  },
  {
    "objectID": "posts/Lesson-23-Confidence-intervals/index.html#statistical-thinking-about-shoe-width",
    "href": "posts/Lesson-23-Confidence-intervals/index.html#statistical-thinking-about-shoe-width",
    "title": "Lesson 23 Resources",
    "section": "",
    "text": "This is a story about the importance of including covariates to answer research questions. Confidence intervals can be “correct” without being informative if appropriate covariates are not considered.\n\nDr. Mary Meyer is a statistics professor at Colorado State University. In 2006, she published an article recounting an episode from family life:\n\nWhen my daughter was in fourth grade, I took her shopping for dress shoes. I was disappointed in the quality of girls’ shoes at every store in the mall. The shoes for boys were sturdy and had plenty of room in the toes. On the other hand, shoes for girls were flimsy, narrow, and had pointed toes. In spite of the better construction for boys, the costs of the shoes were similar! For children the same age, boys had shoes they could run around in, while girls’ shoes were clearly for style and not comfort.\n\n\nUpon complaining about this state of affairs, I was told by sales representatives in two stores that boys actually had wider feet than girls, so needed wider shoes. Being very skeptical, I thought I would test this claim.\n\nWe will return to Dr. Meyer’s project in a little bit. However, for now, imagine how this situation might be addressed by someone who has yet to develop good statistical thinking skills. We will call this imagined protagonist “Mr. Shoebuyer.” Since the salesmen claimed that girls’ feet are narrower than boys, Mr. Shoebuyer heads out to measure the widths of girls’ and boys’ shoes.\nA shoe store provides a convenient place to measure the widths of many different shoe styles. Mr. Shoebuyer gets to the shoe store, heads to the children’s section, and starts measuring. For each shoe on display, he records the shoe width and whether the shoe is for girls or boys. Here are his data:\n\n\n\n\n\nsex\nwidth\n\n\n\n\nG\n9.0\n\n\nG\n9.0\n\n\nG\n8.6\n\n\nG\n9.3\n\n\nB\n9.2\n\n\nB\n9.7\n\n\nB\n8.8\n\n\nB\n9.3\n\n\n\n\n\nOnce back home, Mr. Shoebuyer uses his calculator to find the mean width of the shoes in each group. His results surprise him:\n\n\n\nsex\nmean width\n\n\n\n\nGirls\n8.98 cm\n\n\nBoys\n9.25 cm\n\n\n\nMr. Shoebuyer happens to be your uncle. He knows you are taking a statistics course and asks you to check his arithmetic. Putting on a statistical thinking hat to the effect size of sex on shoe width, you note the absence of a confidence interval. This omission is easy to fix.\n\nShoebuyer_data |&gt; model_train(width ~ sex) |&gt; conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr  .coef  .upr\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 (Intercept)  8.84   9.25  9.66 \n2 sexG        -0.848 -0.275 0.298\n\n\nYour uncle is at the table at Thanksgiving break. “Sorry, Uncle, but you don’t have nearly enough data to conclude that girls’ feet are narrower than boys’.” Translating the confidence interval into plus-or-minus format, you point out that the difference between the sexes is \\(0.275 \\pm 0.6\\) cm. “You’ll need enough data to get that 0.6 margin of error down to something like 0.2 or lower.” You also point out that there might be a better place to collect data than a shoe store. “It’s the feet, not the shoes, that you want to measure.”\nAware of these pitfalls, Dr. Meyer worked with the third- and fourth-grade teachers at her daughter’s school to collect data. Being a statistical thinker, she thought about what data would illuminate the matter before carrying out the data collection. Her data, a sample of size \\(n=39\\), are recorded in the KidsFeet data frame.\n\nKidsFeet |&gt; model_train(width ~ sex) |&gt; conf_interval()\n\n# A tibble: 2 × 4\n  term          .lwr  .coef    .upr\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  8.98   9.19   9.40  \n2 sexG        -0.713 -0.406 -0.0990\n\n\nIn plus-or-minus format, this confidence interval is \\(-0.4 \\pm 0.3\\) cm. Whatever the format, Dr. Meyer’s data provides some evidence that girls’ feet are narrower than boys’.\nAs a statistical thinker, Dr. Meyer knows that even though the foot width is the original quantity of interest, other factors might play a role in the system. For example, boys’ feet might trend longer or shorter than girls’ feet. This possibility should be taken into account by looking at the effect size of sex on width, holding length constant. After all, a shoe buyer first tells the salesperson their foot length (or “size”); the salesperson then brings shoes of that size to try on.\n\nKidsFeet |&gt; model_train(width ~ sex + length) |&gt; conf_interval()\n\n# A tibble: 3 × 4\n  term          .lwr  .coef   .upr\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept)  1.10   3.64  6.18  \n2 sexG        -0.495 -0.233 0.0297\n3 length       0.120  0.221 0.322 \n\n\nAlthough sex is the explanatory variable of primary interest to Dr. Meyer’s question, she knows to include other explanatory variables that might play a role. Such explanatory variables, not of direct interest, are called “covariates.” Dr. Meyer’s statistical expertise led her to consider possible covariates before collecting her data and took the trouble of measuring both foot length and width.\nThe confidence interval on the sexG coefficient includes zero when length is taken into account. Dr. Meyer’s little study provides evidence that even if girls’ shoes tend to be narrower than boys’, the feet inside them have about the same shape for both sexes.\n\n\nSometimes the variables originally collected can be used to formula a new variable that better addresses the research question. In this story, the issue is the shape of shoes. This involves at least two variables: width and length. In the previous narrative we used a general-purpose procedure—adjustment for covariates—to enable us to focus on foot width while taking length into account.\nAn alternative approach is to calculate a new variable that describes both the width and length aspects of foot shape: aspect ratio.\n\nKidsFeet |&gt; \n  mutate(aspect_ratio = width / length) |&gt; \n  model_train(aspect_ratio ~ sex) |&gt; \n  conf_interval()\n\n# A tibble: 2 × 4\n  term           .lwr    .coef    .upr\n  &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  0.359   0.366   0.374  \n2 sexG        -0.0160 -0.00485 0.00631\n\n\nWhen using aspect_ratio, the model need not include length as a covariate; the aspect ratio itself “adjusts” width for length."
  },
  {
    "objectID": "posts/StatChat-10-2023/index.html",
    "href": "posts/StatChat-10-2023/index.html",
    "title": "A Model Statistics Course",
    "section": "",
    "text": "These are presentation notes for the October 2023 StatChat meeting. For more than 15 years, statistical educators in the Twin Cities region of Minnesota have been gathering a half-dozen times a year at StatChat to share comradeship and teaching insights. Among the schools regularly represented are the University of Minnesota, Macalester College, St. Olaf College, Hamline University, Augsburg University, Carleton College, St. Cloud State University, and Minnesota State University Mankato. The slides were slightly revised for a 2024-01-31 presentation “StatsChat” for high-school teachers in Wisconsin.\nAbstract: “Mere Renovation is Too Little Too Late: We Need to Rethink Our Undergraduate Curriculum from the Ground Up” is the title 2015 paper by George Cobb. Honoring George’s challenge, I have been rethinking and re-designing the introductory statistics course, replacing traditional foundations using modern materials and reconfiguring the living and working spaces to suit today’s applied statistical needs and projects. In the spirit of a “model house” used to demonstrate housing innovations, I’ll take you on a tour of my “model course,” whose materials are available free, open, and online. Among the features you’ll see: an accessible handling of causal reasoning, a unification of the course structure around modeling, a highly streamlined yet professional-quality computational platform, and an honest presentation of Hypothesis Testing that puts it in the framework of Bayesian reasoning."
  },
  {
    "objectID": "posts/StatChat-10-2023/index.html#introductions",
    "href": "posts/StatChat-10-2023/index.html#introductions",
    "title": "A Model Statistics Course",
    "section": "Introductions",
    "text": "Introductions\n\nWhat is the most important take-away from your stats course?\nWhat subjects could be dropped without loss?\nAre there course topics that are misleading?\nAre there course topics that are out of date?"
  },
  {
    "objectID": "posts/StatChat-10-2023/index.html#motivation",
    "href": "posts/StatChat-10-2023/index.html#motivation",
    "title": "A Model Statistics Course",
    "section": "Motivation",
    "text": "Motivation\nThe “consensus” Stat 101 is 50 years out of date:\n\nfails to engage issues of causation, covariation, and adjustment\ntoo much emphasis on p-values\nentirely ignores Bayes\nno substantial coverage of risk, risk factors, …\nuses a confusing over-variety of graphic modes (many of which are out-of-date)\ndoesn’t make contact with data science, machine learning, and AI/GPT\n\nI’m happy to discuss the above points anytime, but that’s where I aimed this talk.\nMy objectives:\n\nDemonstrate the extent to which it’s possible to overcome these deficiencies with a complete, practicable, no-prerequisite course.\nProvide a complete course framework, avoiding topic bloat, to which other people can add their own exercises, topics, and examples.\n\nTo this end, there is now a completed draft textbook: Lessons in Statistical Thinking that is free, online.\n\n\n\n\n\n\nJeff Witmer’s approach\n\n\n\nJeff proposes 15 changes*, dividing them into amount-of-effort categories:\n\nChanges You Could Make with Little Effort or Planning. (e.g. “significant” -&gt; discernible)\nChanges to a Course That You Could Implement after Investing a Day or so of Planning. (e.g. emphasize power, not \\(\\alpha\\))\nChanges to a Course That Would Require Quite a Bit of Planning but That Are Worth Considering Nonetheless (e.g. emphasize effect size)\n\nJeff Witmer (2023) “What Should We Do Differently in STAT 101?” Journal of Statistics and Data Science Education link\nAlmost all of which are engaged in Lessons in Statistical Thinking"
  },
  {
    "objectID": "posts/StatChat-10-2023/index.html#style",
    "href": "posts/StatChat-10-2023/index.html#style",
    "title": "A Model Statistics Course",
    "section": "Style",
    "text": "Style\n\nDemonstrate and describe statistical phenomena by causal simulation\n\nExamples:\n\nsampling variation\nconfounders, covariates, colliders, adjustment\n\nStat theory from simulation/wrangling rather than probability/algebra\n\nInformal inference from the very beginning, gradually formalizing it over the semester\nSingle, standard format for graphics: the annotated point plot.\n\nAnnotations for (i) distribution, and (ii) models, including multivariable models.\n\nKeep the software powerful, but simple.\n\nExample: a point plot of Francis Galton’s data on children’s heights versus their parents.\n\n\n\nGalton |&gt; point_plot(height ~ mother + sex)\n\n\n\n\n\n\n\n\n\nExample: The most frequently encountered command has this structure:\n\n\n    Galton |&gt; \n      model_train(height ~ mother + sex) |&gt;\n      conf_interval()\n\n# A tibble: 3 × 4\n  term          .lwr  .coef   .upr\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept) 37.1   41.4   45.8  \n2 mother       0.286  0.353  0.421\n3 sexM         4.87   5.18   5.49"
  },
  {
    "objectID": "posts/StatChat-10-2023/index.html#course-overview",
    "href": "posts/StatChat-10-2023/index.html#course-overview",
    "title": "A Model Statistics Course",
    "section": "Course overview",
    "text": "Course overview\n\nPart 1: Handling data\n\nData frames\nGraphics (data and models)\nWrangling\n\nPart 2: Describing relationships\n\nRegression (incl. categorical and multiple explanatory variables)\nAdjustment\n\nPart 3: Randomness and the unexplained\n\nSignal and noise\nSimulation and DAGs\nProbability models (optional)\nSampling variation and confidence intervals/bands\nLikelihood (optional, prep. for Part 5)\nMeasuring and accumulating risk\n\nPart 4: Causal reasoning\n\nEffect size\nDirected Acyclic Graphs (DAGs), or “influence diagrams”\nCausality/Confounding/Adjustment\nExperiment\n\nPart 5: Hypothetical thinking\n\nBasic Bayes: competing two hypotheses\nHypothesis testing"
  },
  {
    "objectID": "posts/StatChat-10-2023/index.html#how-can-we-fit-more-in-an-already-crowded-course",
    "href": "posts/StatChat-10-2023/index.html#how-can-we-fit-more-in-an-already-crowded-course",
    "title": "A Model Statistics Course",
    "section": "How can we fit more in an already crowded course?",
    "text": "How can we fit more in an already crowded course?\nStreamline!\n\nReduce drag cognitive load.\n\nRepeated use a small number of standard forms\n\none basic graphical pattern: annotated point plot\none basic computational pattern: noun |&gt; verb |&gt; verb |&gt; …\n\nAvoid nomenclature conflicts with everyday words, e.g.\n\n“table” -&gt; data frame\n“case” -&gt; specimen\n“assignment” -&gt; storage\n\n\nUnify t, p into regression modeling, conf. interval/bands in both graphics and models\nKeep number of types of objects small: data frame, model, graphic, simulation\n\nVery small computational footprint, a dozen stat/graphics/wrangling functions.\n\nRemove square roots whenever that’s easy\n\nfocus on variance rather than standard deviation, R^2 rather than r"
  },
  {
    "objectID": "posts/StatChat-10-2023/index.html#part-i-handling-data-6-7-class-hrs",
    "href": "posts/StatChat-10-2023/index.html#part-i-handling-data-6-7-class-hrs",
    "title": "A Model Statistics Course",
    "section": "Part I: Handling Data (6-7 class hrs)",
    "text": "Part I: Handling Data (6-7 class hrs)\n\nLesson 1. Data frames\nData is always in data frames.\n\nColumns: Variables\nRows: “Specimens” / Unit of observation\n\nComputing concepts:\n\nname of data frame, e.g. Galton or Nats\npipe\nfunction()\n\nUsually start with a named data frame, piping it to a function.\n\nNats |&gt; names()\n\n[1] \"country\" \"year\"    \"GDP\"     \"pop\"    \n\n\n\n\nLesson 2. Graphics\nBoth the horizontal and vertical axes are mapped to variables.\nJust one command: point_plot() produces point plot with automatic jittering as needed.\nTilde expression specifies which variable is mapped to y and x (and, optionally, color and faceting).\nGalton |&gt; point_plot(height ~ sex)\nGalton |&gt; point_plot(height ~ mother + father + sex)\n\n\n\n\n\n\n\n\n\n\n\n\nLesson 3. Empirical distributions\n\nGalton |&gt; point_plot(height ~ sex, annot = \"violin\")\n\n\n\n\n\n\n\n\n\n\nLesson 4. Models as graphical annotation\n\nGalton |&gt; sample_n(size=100) |&gt; \n  point_plot(height ~ sex, annot = \"model\", \n             point_ink = 0.1, model_ink=0.75)\n\n\n\n\n\n\n\nGalton |&gt; point_plot(height ~ mother + father + sex, \n                     annot = \"model\", \n                     point_ink = 0.1, model_ink=0.75)"
  },
  {
    "objectID": "posts/StatChat-10-2023/index.html#lesson-5-wrangling",
    "href": "posts/StatChat-10-2023/index.html#lesson-5-wrangling",
    "title": "A Model Statistics Course",
    "section": "Lesson 5: Wrangling",
    "text": "Lesson 5: Wrangling\n[Perhaps use two class days]\nFive basic operations: mutate(), filter(), summarize(), select(), arrange()\n\nNats\n\n# A tibble: 8 × 4\n  country  year   GDP   pop\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Korea    2020   874    32\n2 Cuba     2020    80     7\n3 France   2020  1203    55\n4 India    2020  1100  1300\n5 Korea    1950   100    32\n6 Cuba     1950    60     8\n7 France   1950   250    40\n8 India    1950   300   700\n\n\n\nNats |&gt; filter(year == 2020)\n\n# A tibble: 4 × 4\n  country  year   GDP   pop\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Korea    2020   874    32\n2 Cuba     2020    80     7\n3 France   2020  1203    55\n4 India    2020  1100  1300\n\nNats |&gt; summarize(totalpop = sum(pop), .by=year)\n\n# A tibble: 2 × 2\n   year totalpop\n  &lt;dbl&gt;    &lt;dbl&gt;\n1  2020     1394\n2  1950      780"
  },
  {
    "objectID": "posts/StatChat-10-2023/index.html#lesson-6-computing-recap",
    "href": "posts/StatChat-10-2023/index.html#lesson-6-computing-recap",
    "title": "A Model Statistics Course",
    "section": "Lesson 6: Computing recap",
    "text": "Lesson 6: Computing recap\n[Perhaps merged into a two-day wrangling unit with Lesson 5]\nPipes, functions, parentheses, arguments, …"
  },
  {
    "objectID": "posts/StatChat-10-2023/index.html#lesson-7-databases",
    "href": "posts/StatChat-10-2023/index.html#lesson-7-databases",
    "title": "A Model Statistics Course",
    "section": "Lesson 7: Databases",
    "text": "Lesson 7: Databases\n[Entirely optional]\n\nJoins\nWhy we put related data into separate tables with different units of observation."
  },
  {
    "objectID": "posts/StatChat-10-2023/index.html#part-ii-describing-relationships",
    "href": "posts/StatChat-10-2023/index.html#part-ii-describing-relationships",
    "title": "A Model Statistics Course",
    "section": "Part II: Describing Relationships",
    "text": "Part II: Describing Relationships\nConsistently use explanatory/response modeling paradigm. Introduce models with two or three explanatory variables early in the course.\nUse variance as measure of variation of a variable. (Ask me about the simple explanation of variance that doesn’t involve calculating a mean.)\nUse data wrangling to introduce model values, residuals, …\n\nmtcars |&gt; \n  mutate(mpg_mod = model_values(mpg ~ hp + wt)) |&gt; \n  select(hp, wt, mpg_mod) |&gt; \n  head()\n\n                   hp    wt  mpg_mod\nMazda RX4         110 2.620 23.57233\nMazda RX4 Wag     110 2.875 22.58348\nDatsun 710         93 2.320 25.27582\nHornet 4 Drive    110 3.215 21.26502\nHornet Sportabout 175 3.440 18.32727\nValiant           105 3.460 20.47382\n\n\nThen transition to model coefficients.\n\nmtcars |&gt;\n  model_train(mpg ~ hp + wt) |&gt;\n  conf_interval()\n\n# A tibble: 3 × 4\n  term           .lwr   .coef    .upr\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 34.0    37.2    40.5   \n2 hp          -0.0502 -0.0318 -0.0133\n3 wt          -5.17   -3.88   -2.58  \n\n\nCoefficients are always shown in the context of a confidence interval, even if they don’t yet know the mechanism for generating such intervals.\nDemonstrate mechanism of “adjustment”: Evaluate model holding covariates constant."
  },
  {
    "objectID": "posts/StatChat-10-2023/index.html#part-iii-randomness-and-noise",
    "href": "posts/StatChat-10-2023/index.html#part-iii-randomness-and-noise",
    "title": "A Model Statistics Course",
    "section": "Part III: Randomness and noise",
    "text": "Part III: Randomness and noise\n6-11 class hours, depending on how much spent with named probability distributions. (USAFA engineers want some practice with named distributions: normal, exponential, poisson, …)\n\nSignal and noise\n\n\nSimulations\nStudents construct simple simulations, using them to generate data.\n\nmysim &lt;- datasim_make(\n  x &lt;- rnorm(n),\n  y &lt;- 2 + 3*x + rnorm(n, sd=0.5)\n)\nmysim |&gt; sample(size=4)\n\n# A tibble: 5 × 2\n       x       y\n   &lt;dbl&gt;   &lt;dbl&gt;\n1  0.552  3.97  \n2 -0.675 -0.0812\n3  0.214  3.10  \n4  0.311  2.82  \n5  1.17   5.79  \n\n\n\n\nProbability models\nMostly using simulations.\n\n\nLikelihood\nEarly introduction of the concept of likelihood: probability of data given hypothesis/model.\n\nmain point: distinguish between p(model | data) and p(data | model)\nwe’ll use likelihood in last part of course.\n\n\n\nR2\n\n\nPrediction\n\nThe proper form for a prediction: a relative probability assigned to each possible outcome.\nThe prediction-interval shorthand for (a).\n\n\n\nSampling variation and confidence intervals\n\nRuns &lt;- mysim |&gt; \n  sample(n = 5) |&gt;\n  model_train(y ~ x) |&gt; \n  trials(10) \n\nWarning: The `tidy()` method for objects of class `model_object` is not maintained by the broom team, and is only supported through the `lm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n\nRuns |&gt; select(.trial, term, estimate )\n\n   .trial        term estimate\n1       1 (Intercept)    1.659\n2       1           x    3.164\n3       2 (Intercept)    2.204\n4       2           x    2.950\n5       3 (Intercept)    2.055\n6       3           x    3.318\n7       4 (Intercept)    1.771\n8       4           x    3.141\n9       5 (Intercept)    2.156\n10      5           x    2.920\n11      6 (Intercept)    2.157\n12      6           x    3.324\n13      7 (Intercept)    2.091\n14      7           x    2.833\n15      8 (Intercept)    2.446\n16      8           x    2.794\n17      9 (Intercept)    1.892\n18      9           x    2.399\n19     10 (Intercept)    2.053\n20     10           x    2.694\n\nRuns |&gt; summarize(var(estimate), .by = term)\n\n         term var(estimate)\n1 (Intercept)       0.05117\n2           x       0.08504\n\n\nDemonstrate that variance scales as 1/n. \n\n\nRisk\n(2 or 3 day unit)\nDefinition of risk, risk factors, baseline risk, risk ratios, absolute change in risk.\nUse absolute change for decision making, but use risk ratios and odds ratios for calculations.\nRegression when response is a zero-one variable.\n\nWhickham |&gt; point_plot(outcome ~ smoker, annot=\"model\", \n                       model_ink = 1)\n\n\n\n\n\n\n\nWhickham |&gt; point_plot(outcome ~ age + smoker, annot=\"model\")"
  },
  {
    "objectID": "posts/StatChat-10-2023/index.html#part-iv-causal-modeling",
    "href": "posts/StatChat-10-2023/index.html#part-iv-causal-modeling",
    "title": "A Model Statistics Course",
    "section": "Part IV: Causal modeling",
    "text": "Part IV: Causal modeling\n\nEffect size\nRatio of (change in output) to (change in input).\nPhysical units important.\n\n\nDAGs\nReading “influence diagrams”\n\n\n\n\n\n\n\n\n\n\n\nCausality\nconfounding, covariates, and adjustment\nChoosing covariates based on a DAG\n\n\nExperiment\nexperiment interpreted as re-wiring of DAGs: requires intervention"
  },
  {
    "objectID": "posts/StatChat-10-2023/index.html#part-v-hypothetical-thinking",
    "href": "posts/StatChat-10-2023/index.html#part-v-hypothetical-thinking",
    "title": "A Model Statistics Course",
    "section": "Part V: Hypothetical thinking",
    "text": "Part V: Hypothetical thinking\n\nStrong emphasis on the idea of hypothesis.\n\nYour turn: Please define hypothesis.\n\nWhat we want: p(hypothesis | data)\nWhat we have: hypothesis |&gt; simulation |&gt; data summarized as a Likelihood.\nQuestion: How do we calculate what we want.\n\n\nBayes framework\n\nSetting: medical screening. Test result + or -.\nWe put two hypotheses into competition based on the test result.\nLikelihoods we can measure from data:\n\np(+ | Sick) aka “sensitivity”\np(+ | Healthy) aka “false-positive rate” translated to “specificity”\n\nSetting: prevalence(Sick)\nCalculation\n\nGraphically\nAlgebra from the graph\nFormula in Likelihood-ratio form\n\n\n\\[odds(Sick|+) = \\frac{p(+ | Sick)}{p(+ | Healthy)} \\ odds(prevalence)\\]\n\n\nNull hypothesis testing\nNull is “Healthy.”\nWe have no claim about \\(prevalence\\).\nIf we have no claim about \\(p(+ | Sick)\\), we are more inclined to conclude \\(Sick\\) if \\(p(+ | Healthy)\\) is small.\n\n\\(p(+ | Healthy)\\) is p-value.\n\n\n\nNeyman-Pearson\nNull is “Healthy.” Alternative is “Sick”.\nWe have no claim about \\(prevalence\\), but we have the ability to estimate \\(p(+ | Sick)\\).\nInclined to conclude \\(Sick\\) if \\(p(+ | Healthy)\\) is small (like HNT) and \\(p(+ | Sick)\\) is small. \\(p(+ | Sick)\\) is the power of the test.\n\n\nGotcha’s in HT\n\nWithout power, can’t say what constitutes a big p-value.\nEstimation of p suffers greatly from sampling variation. No good reason to think that p &lt; 0.01 is any different from p &lt; 0.05.\nSensible decisions require knowledge of prevalence.\nEffect size, not p-value, is needed to interpret practical importance of results."
  },
  {
    "objectID": "posts/StatChat-10-2023/index.html#objects-and-operations",
    "href": "posts/StatChat-10-2023/index.html#objects-and-operations",
    "title": "A Model Statistics Course",
    "section": "Objects and operations",
    "text": "Objects and operations\n\nData frame\nData graphics (as distinct from “infographics”)\nStatistical model\nSimulation\n\nOperations for all students\n\nData wrangling (simplified)\nAnnotated point plot of variables from a data frame\nModel training\nModel summarization\n\nOperations used in demonstrations (and suited to some students)\n\nSimulation (in demonstrations)\nIteration and accumulation (in demonstrations)\n\nComputations on variables are always inside the arguments of a function taking a data frame as an input.\nTilde expressions for models and graphics."
  },
  {
    "objectID": "posts/StatChat-10-2023/index.html#resources",
    "href": "posts/StatChat-10-2023/index.html#resources",
    "title": "A Model Statistics Course",
    "section": "Resources",
    "text": "Resources\n\nTextbook: Statistical Inference via Data Science by Chester Ismay and Albert Y. Kim\nTextbook: Lessons in Statistical Thinking by Danny Kaplan\n\nAssociated R package: {LST}\n\nUSAFA Math 300Z course outline, instructor notes, etc.\nJeff Witmer (2023) “What Should We Do Differently in STAT 101?” Journal of Statistics and Data Science Education link"
  },
  {
    "objectID": "posts/Triangles/index.html",
    "href": "posts/Triangles/index.html",
    "title": "Pythagorean statistics",
    "section": "",
    "text": "DRAFT, DRAFT, DRAFT\n\n\n\n\n\n\nInstructors should note the similarity of the partitioning of variances to the Pythagorean Theorem. With right triangles, the square lengths of the legs add to the square length of the hypotenuse: \\(A^2 + B^2 = C^2\\). This is not a coincidence.\nSeen as vectors, the response variable is the hypotenuse of a right triangle whose legs are the model values and residuals. You can see that the legs are perpendicular from the dot product of the model values and the residuals:\nREPLACE THIS WITH A STAND-ALONE EXAMPLE\nMAYBE CALCULATE THE VARIANCE TO SHOW THE ANALOGY TO THE RIGHT TRIANGLE. “Regarding the residuals and the model values as vectors, we can confirm the analogy to the right triangle by calculating their dot product. It’s zero, indicating that the vectors are perpendicular.”\n\nModel1 |&gt;\n  summarize(dotproduct = sum(modval * resid))"
  },
  {
    "objectID": "posts/Pithy-R/index.html",
    "href": "posts/Pithy-R/index.html",
    "title": "Reducing R friction with the {LSTbook} package",
    "section": "",
    "text": "The “learning curve” and “overhead” are standard metaphors for the difficulties of learning something new. R is described as having a steep learning curve and high overhead. I prefer another metaphor: “friction.” Over the last 20 years, I have introduced students and instructors to ways of using R with less friction, for example, with the {mosaic} set of packages. As R and the R ecosystem continue to develop, the potential to reduce friction becomes greater. Many of these developments are widely known in the R community. Examples are {ggplot2}, RStudio, Shiny, the tidyverse, and implementations for data wrangling such {dplyr}.\nThis post is about my latest work to reduce friction, focusing on the structure of R commands for introducing statistical thinking and engaging data science. This new work forms part of a project to reform the set of ideas taught in applied introductory statistics, discarding outdated methods and encompassing the “advanced” methods and concepts widely used in applied research and data science. That reform is presented in Lessons in Statistical Thinking (LST), a free, online, open-source text for a model course.\nAn important technique for reducing friction is to make as small as possible the number of object types. In LST, there are just three object types: data frames, model objects, and DAGs.\nAnother way to reduce friction is to avoid forcing the user to make choices than can sensibly be made automatically. An example of this in LST is the construction of data graphics. LST standarizes on the “annotated point plot” as produced by the LSTbook::point_plot() function. The user need only specify a data frame and a tilde expression that identifies which are the explanatory variables and which is the response variable. Given this information, point_plot() makes sensible choices. For example, using the well-known Palmer penguin data, Figure 1 shows a plot of bill_length in terms of two explanatory variables.\n\n\n\n\nPenguins |&gt; point_plot(bill_length ~ mass + flipper)\n\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\nThe response variable is always placed on the y axis. The first explanatory variable is mapped to x, the second (if any) to color and a third (if any) to facet. When continuous variables are being mapped to color or facet, they are automatically discretized in order to make the graphic easier to interpret. Might an experienced user want to use a continuous scale of color? Of course. But for the beginning user it’s better to use a more easily interpreted scale.\nWhen discrete-valued, categorical variables are being plotted, jittering is automatically applied to the spatial coordinates, as in Figure 2:\n\n\n\n\nPenguins |&gt; point_plot(sex ~ flipper + species, annot = \"model\")\n\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\nFriction is often introduced by the need to make syntactical choices in writing commands. Examples often seen in the use of R in introductory statistics include the various ways of storing arrays of data, indexing subsets of data, and adding new components to data. These include dollar signs, square braces (both single and double), commas encasing blank space, and so on.\nFriction is also introduced by having a large number of object types for similar purposes (e.g. vectors, lists, data frames). An example of this prevalent in introductory statistics is the use of different functions to perform essentially the same operation, e.g. t.test(), prop.test(), lm(), glm().\nAnother source of friction is the use of differently named functions (and different argument schemes) when the issue is really the details of the operation to be performed, not the type of operation.\n\nA large number of basic object types. For instance, introductions to R often include vectors, lists, data frames,\nA variety of syntaxes for invoking operations on arguments.\nThe clarity of the syntax, for instance spreading apart typographically the arguments to a function.\nA large number of named operations.\nFailure to use analogous syntax for analogous operations. For example, in {ggplot2}, named arguments are used to specify how variables are mapped to aesthetics (e.g. aes(x = age, y = height)), while in modeling, tilde expressions (“R formulas”) are used to describe the roles of response and explanatory variables (e.g. lm(height ~ age))."
  },
  {
    "objectID": "posts/Why-LST/index.html",
    "href": "posts/Why-LST/index.html",
    "title": "Reforming Stat 101 with Lessons in Statistical Thinking",
    "section": "",
    "text": "Lessons in Statistical Thinking (LST) is a new text for helping college-level students understand, read, and work with applied statistics as it is practiced today. It is part of a broader project that aims to reform the Stat 101 course so widely taught as the college-level introduction to statistics. Some readers will already be aware of the need for reform; a short case is made in Section 3.\nA process of winnowing out topics is central to reforming Stat 101. Instructors often claim that Stat 101 is already overcrowded, leaving no room for additional content reflecting today’s applied statistics practice. Common sense suggests cleaning out low-importance topics. A small set of examples of such topics were given in the ASA 2016 College GAISE report in the section titled “Suggestions for topics that might be omitted from introductory statistics courses.”\nLST is the product of a more stringent process to identify a small set of essential topics. Rather than starting with the topics already found in Stat 101, I considered broad areas of “applied statistics as it is practiced today.” Two examples are causal reasoning and Bayesian inference. I worked backward from these aspirational topics to the foundations needed to support them. For instance, a basic part of causal reasoning is adjustment for covariates. Both “adjustment” and “covariates” need to build on foundations. And so on.\nSimilarly, techniques such as data wrangling are an important component of today’s applied statistics practice. In forming LST, I looked for a minimal set of foundations for a creditable introduction to data wrangling. These include an ability to compute and the relational operations used in wrangling. But once it is accepted that readers of LST will develop meaningful (if basic) skills in data wrangling it becomes possible to use these skills as part of the exposition of statistics itself.\nOne advantage of treating causality in depth is reflected in an approach taken in LST to present statistical theory. For instance, statistical thinking involves an important distinction between accuracy and precision. Stat 101 emphasizes precision; the extensive coverage of confidence intervals is an example. But Stat 101 says little about accuracy beyond “take a random sample.” Yet even news reports of applied statistical work routinely include phrases like “after adjusting for ….”\nLST can draw on causal inference concepts such as causal networks and directed acyclic graphs (DAGs) when illustrating accuracy. With a causal network, the student can see the mechanics exactly. Drawing simulation data from the causal network enables a demonstration of covariates and confounding and what methods, such as adjustment for covariates, can and can’t contribute to accuracy."
  },
  {
    "objectID": "posts/Why-LST/index.html#streamlining-explanations",
    "href": "posts/Why-LST/index.html#streamlining-explanations",
    "title": "Reforming Stat 101 with Lessons in Statistical Thinking",
    "section": "Streamlining explanations",
    "text": "Streamlining explanations\nIn addition to minimizing the topic set needed to reach high-level objectives, LST reconsiders the path of introducing statistical basics. To take an example, consider “standard deviation.” Typically, the Stat 101 lead-up to standard deviation starts with “distributions” and the graphical modes for displaying distributions. One of these is the histogram, which then gets smoothed into a continuous density picture of the normal distribution. Then the graphical features of this picture such as the location of the peak and the width of the bell-shaped mountain are associated with statistical word: mean and standard deviation. It’s natural from there to discuss how much of the normal distribution is contained within \\(\\pm 1\\) or \\(\\pm\\) 2 standard deviations of the peak.\nIn LST, the path to “standard deviation” is set by reference to a high-level objective: recognizing variation. It’s easy to see variation in a data frame: the differing values of a variable from row to row. But how do you quantify the “amount” of variation? The standard measure of variation in LST is the variance. But there is no need to invoke the normal distribution to understand variance, nor to build it up as the square of the standard deviation. Instead, the explanation of variance starts with a simple question: Given two values of a variable, by how much do they vary? One nice answer is to take the difference in values and then square.\nWhat about the amount of variation in three values of a variable? Generalize the above square difference by calculating the square difference between every pair of the three values. There are three such square differences, so take the average as the measure of variation. A similar process can be followed to measure the variation in any number of values.\nThe result of this simple process has a name that dates from very early in statistics: 1885. More important for us today is that the result is precisely twice the variance. So divide the average of square pairwise differences by two and you have the variance. There’s no need to refer to the mean or the shape of the normal distribution. Just a common sense average of differences, the only part of which a student will ask for a justification is the squaring.\nI think there are cognitive advantages to emphasizing variance rather than standard deviation. Among these are the physical units of a variance: the square of the units of the variable itself. This helps to emphasize that variation in a quantity is a different kind of thing than the quantity itself. More important is the savings that come from not needing to introduce the normal distribution early in the course.\nA more important conceptual streamlining in LST involves unifying the various statistical tests in Stat 101 around a single method: regression modeling. The idea of “response” and “explanatory” variables is presented as early as Lesson 2. The Stat 101 distinction between means and proportions—the objects of t- and p-tests respectively—is not at all central. Both t- and p- fit into the framework of regression. In particular, both are instances where there is a single explanatory variable that is categorical.\nSimilarly, there is no need to emphasize a distinction between so-called one-sample and two-sample settings; both fit nicely into the regression framework. There is a cognitive and motivational gain, I think, in starting with describing relationships between two (or more) variables. The one-sample setting can be handled later as corresponding to an explanatory variable with no variation.\nLST also streamlines statistical graphics. There is just one fundamental form of data graphic: the annotated point plot. Each point itself refers to a single row of a data frame. In contrast, the annotations refer to the collective properties of the rows. Two types of annotations are available: a “violin” to display density, and a “model” to display trends. From the very start, the model annotations incorporate the uncertainty due to sampling variation. When confidence intervals are introduced in later lessons, the student already has an image of the uncertainty that they quantify. p-values appear only in the last lesson, placed there because (1) a statistically literate person needs to know what they are about, but (2) taking seriously how the 2016 ASA Statement on p-values points to the emphasis on p-values as unfortunate. In any event, students encounter hypothesis testing informally at a much earlier point, checking whether confidence intervals include zero when assessing whether a relationship has been detected or not.\nComputing in LST uses R, but the commands are designed around a minimal notation set and aim to be readable, pithy, and easy to emulate. There are no dollar signs, no square or curly braces. The built-in R pipe notation is used systematically to construct commands, and the outputs of modeling summaries are always data frames, hence amenable to the wrangling procedures covered early in LST. Concise modeling and graphics commands, available from the {LSTbook} package on CRAN, follow similar syntactical patterns. For instance, here is a command for a graphical presentation of a model:\n\nPenguins |&gt; \n  point_plot(bill_length ~ mass + sex + species, annot = \"model\")\n\n\n\n\n\n\n\n\nThe above command is very similar to a command to generate a quantitative presentation of a model:\n\nPenguins |&gt;\n  model_train(bill_length ~ mass + sex + species) |&gt;\n  conf_interval()\n\n# A tibble: 5 × 4\n  term                  .lwr    .coef     .upr\n  &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)      28.4      31.1     33.7    \n2 mass              0.000980  0.00175  0.00253\n3 sexmale           1.81      2.52     3.23   \n4 speciesChinstrap  9.31      9.96    10.6    \n5 speciesGentoo     5.08      6.28     7.48"
  },
  {
    "objectID": "posts/Why-LST/index.html#the-broader-lst-project",
    "href": "posts/Why-LST/index.html#the-broader-lst-project",
    "title": "Reforming Stat 101 with Lessons in Statistical Thinking",
    "section": "The broader LST project",
    "text": "The broader LST project\nMy primary goal with LST is to help students and instructors engage with “applied statistics as it is practiced today.” The Lessons in Statistical Thinking text was written as a demonstration of the extent to which this goal can be met with a one-semester, no-prerequisite course.\nMore fundamentally, LST embraces what I see as a coherent, connected, and minimal set of topics needed to achieve the goal. Necessarily, not all important topics can be covered in a one-semester, no-prerequisite course. Notable omissions include time series, non-parametrics, and experimental design, among many others. Also omitted, inevitably, are topics that individual instructors think central to how they conceive of statistics, or beautiful in their own right, or addressing specific important applied contexts.\nI encourage the instructor to think of LST as a starting point in an ongoing conversation about what topics should be central to the college-level statistics curriculum. And I propose a test for such centrality. Suppose your students had a realistic mastery of the LST topics at a level that might be expected in a one-semester course. Would such students be in a good position to learn topics outside of LST that the instructor individually thinks of as important? For instance, if you teach a course on Bayesian inference or on causal inference, would the background provided by LST be superior to the background gained in a conventional Stat 101 course?\nOr, consider any topic or example context you consider essential to a creditable introduction to statistics. Could you adequately address that topic or context using the methods and concepts already in LST as a foundation?\nInsofar as the answers to the previous questions are yes, the topics in LST might be considered a standard pre-requisite for students entering into more advanced study. It would be valuable to have a better standard than what we use now to express prerequisites, e.g., “a course in statistics at the AP level.”\nIs LST suitable to serve as such a standard? The project is currently the product of one person, albeit one who participates actively in the community discussing change in Stat 101. Perhaps if I could think more clearly about the cognitive foundations of applied statistics, there would be additional topics to add to the standard. And there are likely topics that reflect my idiosyncratic interests and the particular applications of statistics that I have encountered in my career but are not important to a broad community.\nLST is a free, online, open-source text. I think it can serve as a framework to support the ideas of a broader community of statistics authors. I am open to any mode of publishing material and acknowledging authorship that can sustain and advance the project. If it helps, I do not need to be considered the primary author. (Perhaps it would be worthwhile to adopt a system such as the Bourbaki pseudonym for collective authorship that was active in the first half of the 20th century.) If an author believes that worthwhile extensions require participation of a commercial entity, I’m happy for my contributions to LST to be used appropriately by that entity. Similarly, if a potential author needs help in order to frame her ideas in a way compatible with LST, I am available to provide such help.\nAnother aspect of the broader project is addressing the limited preparedness of many current statistics instructors. I suspect many instructors will be unfamiliar with important topics in LST. This is likely because a substantial part of the introductory statistics professorship learned statistics by repeatedly teaching Stat 101.\nHow best to bring such instructors on board while acknowledging the realities of limited time and lack of support from their home institution? (And many teachers of Stat 101, particularly adjuncts, don’t even have a clear home institution that accepts a responsibility for ensuring continuing professional development.) My experience is that seminars and webinars are only minimally effective at helping instructors make a change in their teaching. Short courses at professional meetings are little better. Still, participants in such courses quickly saturate their ability to assimilate new information and, more damagingly, have no way to deal with the academic politics that so often stands in the way of innovation.\nBetter alternatives might be a two-hour per week, semester-long walk-through of the topics, done primarily in discussion format and then transformed into an asynchronous web resource. Short courses are more likely to be successful if the people involved can shape institutional decisions. In very small programs, one or two instructors may control the curriculum. With larger programs, the short course is best provided at the initiative of administrative leaders and department chairs and involves a large fraction of the statistics faculty as well as representatives of allied disciplines that require statistics of their students.\nPlease let me know if you might be interested in collaborating on the broad project as a potential author of extensions or as a leader or participant in outreach efforts. I’m also interested in hearing from colleagues who see LST as missing potential opportunities or who have ideas for further streamlining topics."
  },
  {
    "objectID": "posts/Why-LST/index.html#sec-reform-case",
    "href": "posts/Why-LST/index.html#sec-reform-case",
    "title": "Reforming Stat 101 with Lessons in Statistical Thinking",
    "section": "A case for reforming Stat 101",
    "text": "A case for reforming Stat 101\nA reasonable outsider might presume that orienting students to “applied statistics as it is practiced today” is the purpose of the “Stat 101” courses that are ubiquitous in colleges and universities. There are already many textbooks for Stat 101, some of which have been refined over 10 to 30 years of successive editions. These books employ a variety of pedagogical approaches and use different examples, but they are mainly similar in the set of topics covered, a canonical set recognizable to all instructors teaching Stat 101.\nRegrettably, the canonical Stat 101 topics are not strongly oriented to “applied statistics as it is practiced today.” Instead, the topics cover mainly techniques and theory from the early 20th century with a handful of additions from as recently as 1980. To the typical Stat 101 instructor, often trained in mathematics, the focus on historically early topics makes sense; that is the orientation of other components of the math curriculum such as algebra and calculus.\nI have asked many mathematics instructors what parts of the algebra, calculus, and trigonometry curriculum reflect developments in the last century. The typical response is that the question is irrelevant; those topics were fully developed long ago.\nHowever, applied statisticians, when asked what techniques they have used that are new in the last 50 years, can construct a long list. This will include machine learning, causal inference, renewed interest in Bayesian inference, logistic regression, statistical computing, data wrangling, bootstrapping, new types of graphics, and so on.\nStat 101 fails to address these topics. It even fails to address statistical techniques that are ubiquitous in applied work, such as adjustment for covariates. There are a variety of explanations for this failure. The course is already too crowded and the textbooks too thick; there’s no space for additional topics. Or, students are not ready to handle such “advanced” topics. Or, these topics often involve “black box” computing, but our goal is to understand the basic ideas underlying statistics. Or even, Bayesian and causal inference are not broadly accepted as legitimate statistical methods.\nI think that such “explanations” are merely justifications for not reforming Stat 101. Is the course too crowded? Then, throw some items overboard. The new topics are too advanced? Figure out how to teach or motivate them in an accessible way. Is too much computing needed? Make computing more straightforward and accessible.\nThe Stat 101 topics are also stabilized by a lack of contact of instructors with a broader statistics education community. My personal experiences working with hundreds of instructors from a wide variety of institutions suggest that most instructors are yet aware of significant developments from 2016, such as the ASA GAISE report or the ASA Statement on p-values. An instructor who has not met the extensive critique of p-values is unlikely to prioritize reducing the coverage of p-values in his or her course."
  },
  {
    "objectID": "posts/Trout-perch-allometrics/index.html",
    "href": "posts/Trout-perch-allometrics/index.html",
    "title": "Nonlinear Modeling: Something Fishy",
    "section": "",
    "text": "This example lesson is modified from a lesson plan posted in April 2021 on the STEW site, written by Douglas Whitaker of Mount Saint Vincent University."
  },
  {
    "objectID": "posts/Trout-perch-allometrics/index.html#overview-of-lesson",
    "href": "posts/Trout-perch-allometrics/index.html#overview-of-lesson",
    "title": "Nonlinear Modeling: Something Fishy",
    "section": "Overview of Lesson",
    "text": "Overview of Lesson\nIn this lesson students explore nonlinear regression models to explain fish weight using fish length, using both transformation of the response variable and polynomial regression. Geometric interpretations of variables are leveraged to suggest nonlinear models to fit. The intention of this lesson is for students to perform two or three linear regression analyses that feel like others that they have done before: the difference is that they draw on prior knowledge of geometric/physical relationships to suggest a modification to the first analysis to improve it. Because most of the nonlinear models considered in this lesson have only a single predictor variable, students’ familiarity with simple linear regression can be extended to nonlinear modeling. If students are familiar with multiple linear regression, then two additional polynomial regression models can be included. (author: Whitaker)\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNote: Throughout the lesson, the term weight is used for what is more properly described as mass. The original dataset uses the term weight but measures the fish using metric units for mass. The distinction is irrelevant to the lesson, but it would be reasonable to make the changes throughout the lesson if the distinction matters to you. (author: Whitaker)"
  },
  {
    "objectID": "posts/Trout-perch-allometrics/index.html#description-of-dataset",
    "href": "posts/Trout-perch-allometrics/index.html#description-of-dataset",
    "title": "Nonlinear Modeling: Something Fishy",
    "section": "Description of Dataset",
    "text": "Description of Dataset\nScientists are interested in monitoring the health of trout perch in the Oil Sands Region of Canada.1,2 As part of a larger study, fish were collected from the Athabasca River and Peace River, and several characteristics were measured, including the Weight of each fish (in grams) and the Length of each fish (in millimeters). The dataset used in this analysis includes data from 2088 trout perch. (author: Whitaker)\nHere are the data, in the form of a data frame named Trout (author: kaplan)"
  },
  {
    "objectID": "posts/Correlation-is-causation/index.html",
    "href": "posts/Correlation-is-causation/index.html",
    "title": "Correlation and causation",
    "section": "",
    "text": "Rough draft\nA dictionary is a starting point for understanding the use of a word. Here are four definitions of “correlation” from general-purpose dictionaries.\nAll four definitions use “connection” or “relation/relationship.” That is at the core of “correlation.” Indeed, “relation” is part of the word “correlation.” One of the definitions uses “causes” explicitly, and the everyday meaning of “connection” and “relation” tend to point in this direction. The phrase “one thing changes as the other does” is close to the idea of causality, as is “interdependence.:\nThree of the definitions use the words “vary,” “variable,” or “changes.” The emphasis on variation also appears directly in a close statistical synonym for correlation: “covariance.”\nTwo of the definitions refer to “chance,” that correlation “is not caused by chance,” or “not expected on the basis of chance alone.” These phrases suggest to a general reader that correlation, since not based on chance, must be a matter of fate: pre-determination and the action of causal mechanisms.\nWe can put the above definitions in the context of four major themes of these Lessons:\nCorrelation is about relationships; the “correlation coefficient” is a way to describe a straight-line relationship quantitatively. The correlation coefficient addresses the tandem variation of quantities, or, more simply stated, how “one thing changes as the other does.”\nTo a statistical thinker, the concern about “chance” in the definitions is not about fate but reliability. Sampling variation can lead to the appearance of a pattern in some samples of a process that is not seen in other samples of that same process. Reliability means that the pattern will appear in a large majority of samples.\nOne of the better explanations of “correlation” appears in an 1890 article by Francis Galton, who invented the correlation coefficient. Since the explanation is more than a century old, some words will be unfamiliar to the modern reader. For example, a “clerk” is an office worker. An “omnibus” is merely a means of public transportation: a “bus.”\nFrancis Galton’s 1890 example of the clerks on the bus introduces “correlation” as a causality story. The bus trip causes variation in commute times. Two clerks riding the same bus will have correlated commute times. In the dictionary definitions of “correlation” at the start of this post, the words “connection,” “relationship,” and “interdependence” suggests causal connections.\nHistorically, the rise of the expression “correlation does not imply causation”—Figure 1 shows the ngram since the 1888 invention of the correlation coefficient—comes after the peak in the use of the word “correlation.”\nFigure 1: Google NGram showing the rise in the use of the phrases “correlation does not imply causation,” and “correlation is not causation” in recent decades.\nThe first documented use of the phrase is from 1900. It comes in a review of the second edition of a book, The Grammar of Science, by Karl Pearson (whom we have met before in this Lesson).\nThe Grammar of Science is a metaphysically oriented prescription for a new type of science. It posited that sciences such as physics or chemistry unnecessarily drew on metaphors for causation, such as “force.” Instead, the book advocated another framework as more appropriate, eschewing causation in favor of descriptions of “perceptions” with probability.\nPearson illustrates his antipathy toward causation with an example of an ash tree in his garden:\nIt should not be surprising that the field of statistics, which uses probability extensively as a description and that developed correlation as a measure of probability, would advocate for more general use of its approach. In this spirit, I read “correlation does not imply causation” as “our new science framework of probability and correlation replaces the antiquated framework of causation.” Outside of statistics, however, probability is merely a tool; causation does indeed have practical use. All the more so for decision-makers.\nInsofar as the dictionary definitions of correlation suggest a causal relationship, they are at odds with the statistical mainstream, which famously holds that “correlation does not imply causation.” This view is so entrenched that it appears on tee shirts, one style of which is available for sale by the American Statistical Association.\nThe statement “A is not B” can be valid only if we know what A and B are. We have a handle on the meaning of “correlation.” So what is the meaning of “causation?”\nDictionaries define “causation” using the word “cause.” So we look there for guidance.\nInterpreting these definitions requires making sense of “give rise to,” “makes happen,” or “happens as a result.” All of them are synonyms for “cause.”\nThis circularity produces a muddle. Centuries of philosophical debate have yet to clarify things much.\nStill, we can do something. The point of view of these Lessons is to support decision-making. Causation is a valuable concept for decision-making, particularly in cases where the decision-maker is considering an intervention. With this as an anchor, a pragmatic definition of “causation” is available:\nWhether or not a definitive demonstration is feasible is not directly relevant to the decision-maker. A decision-maker acts under the guidance of one or more hypotheses. A good rule of thumb for decision-makers is to be guided only by plausible hypotheses. Whether a hypothesis is plausible is a matter of informed belief. A definitive demonstration should sharpen that belief. If no such definitive demonstration is available, the decision-maker must rely on alternative sources for belief. Austin Bradford Hill (1898-1991), an epidemiologist and eminent statistician, famously published a list of nine criteria that support belief in a causal hypothesis.\nUsing my definition of causation, and in marked disagreement with many statisticians, I submit that\n“Correlation implies causation” is not the same as saying, “A correlation between A and B implies that A causes B.” That statement is false. For instance, it might be instead that B causes A. Alternatively, there might be a common cause C for both A and B. Or, C might be a collider between A and B.\nThere is no mechanism to produce correlation that I am aware of, other than the sources of spurious correlation described previously, that does not involve causation in some way."
  },
  {
    "objectID": "posts/Correlation-is-causation/index.html#so-why-do-many-statisticians-say-different",
    "href": "posts/Correlation-is-causation/index.html#so-why-do-many-statisticians-say-different",
    "title": "Correlation and causation",
    "section": "So why do many statisticians say different?",
    "text": "So why do many statisticians say different?\n\n\n\n\n\n\n\n\n\nHistorically, the rise of the expression “correlation does not imply causation”—Figure 1 shows the ngram since the 1888 invention of the correlation coefficient—comes after the peak in the use of the word “correlation.”\n\n\n\n\n\n\n\n\nFigure 2: Google NGram showing the rise in the use of the phrases “correlation does not imply causation,” and “correlation is not causation” in recent decades.\n\n\n\n\n\nThe first documented use of the phrase is from 1900. It comes in a review of the second edition of a book, The Grammar of Science, by Karl Pearson (whom we have met before in this Lesson).\nThe Grammar of Science is a metaphysically oriented prescription for a new type of science. It posited that sciences such as physics or chemistry unnecessarily drew on metaphors for causation, such as “force.” Instead, the book advocated another framework as more appropriate, eschewing causation in favor of descriptions of “perceptions” with probability.\nIt should not be surprising that the field of statistics, which uses probability extensively as a description and that developed correlation as a measure of probability, would advocate for more general use of its approach. In this spirit, I read “correlation does not imply causation” as “our new science framework of probability and correlation replaces the antiquated framework of causation.” Outside of statistics, however, probability is merely a tool; causation does indeed have practical use. All the more so for decision-makers."
  },
  {
    "objectID": "posts/Correlation-is-causation/index.html#footnotes",
    "href": "posts/Correlation-is-causation/index.html#footnotes",
    "title": "Correlation and causation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFrancis Galton (1890) “Kinship and Correlation” The North American Review 150(401) URL↩︎\nWe will consider a “direct causal link” to be a form of causal path.↩︎"
  },
  {
    "objectID": "posts/Likelihood/index.html",
    "href": "posts/Likelihood/index.html",
    "title": "*Instructor’s Note: Likelihood",
    "section": "",
    "text": "When I introduce the statistical concept of likelihood in instructor workshops covering Lessons in Statistical Thinking, there are two reactions that I see almost every time.\n\nIsn’t likelihood too advanced a topic for introductory statistics? It belongs in the junior-level mathematical statistics course.\nWhat is likelihood? This is particular prevalent among instructors whose main contact with statistics has been teaching an introductory course, as so often happens when mathematicians are drafted to teach intro stats.\n\nThe purpose of this post is two-fold: to explain why likelihood is an important topic for understanding statistics and to point out my approach to teaching likelihood which I think makes the concept easier to grasp than the standard exposition. If you want an introduction to likelihood first, I encourage you to look at Lessons 16, 26, 27, and 28 of LST.\nYou may know that the history of statistics is full of disputes between the so-called “frequentists” and the “Bayesians.” (Such disputes occasionally crop up even today, but I think the broad trend is to see them as related, but complementary aspects of statistical inference and method.) Introductory statistics textbooks are almost always written from the frequentist point of view, and many don’t even mention Bayes. On the other hand, data scientists and engineers tend not to dwell on frequentist ideas, but treat the Bayesian approach as useful for decision making.\nBoth frequentist and Bayesian perspectives place great emphasis on the idea of likelihood. For instance, the frequentists’ p-value is a likelihood and frequentist estimates are often made using a maximum likelihood procedure. For Bayesians, likelihood functions are the means by which a prior is updated into a posterior.\nAccepting the previous paragraph as showing that likelihood is a mainstream topic broadly in statistical thinking, I to considering whether likelihood is too difficult for introductory students.\nI think many instructors will be sympathetic to the claim that Bayes Rule, in the format commonly used, is easy to derive but hard to interpret. Here’s the common format that I mean, that relates conditional and marginal probabilities:\n\\[p(A | B) = \\frac{p(B | A) p(A)}{p(B)}\\] How to find the quantity \\(p(B)\\) is often a mystery to students. One common explanation is, “Don’t worry about \\(p(B)\\) is just a normalizing factor.” How to calculate this normalizing factor? The answer, in practice, is that \\[p(B) = p(B|A) p(A) + p(B| not\\ A) p(not\\ A)\\ .\\]\nWhat does this have to do with likelihood? Both \\(p(B | A)\\) and \\(p(B | not\\ A)\\) are likelihoods.\nAt least three things get in the way of assimilating the meaning of a likelihood from the Bayes Rule formula.\n\n“Likelihood” seems like an arbitrary name assigned to certain configuration of conditional probabilities but not others. And why isn’t \\(p(A|B)\\) called a likelihood? (It’s not, it’s a posterior probability. But it looks very much like a likelihood.)\nIn for formula, A and B look like they are the same thing, or perhaps different possible outcomes of the same event: either A or B. But really, A and B refer to two different kinds of things. For example, B might be whether a patient is sick or healthy while A is whether the clinical test comes up positive or negative. That is, there are multiple possibilities (e.g. sick/healthy) for the thing called A and a different set of multiple possibilities for the thing call B (e.g. + or - test result). This is implicit in the formula; there’s nothing to remind the reader that A is one kind of thing and B is another.\nIt’s not too hard to introduce the concept of joint, conditional, and marginal probabilities with a two-by-two table of counts. Pointing out that a likelihood is a kind of conditional probability is a tempting pedagogical route: First nail conditional probability using tables or a graph relating areas, then explain that a likelihood is a conditional probability of the same sort as the ones that were so easy when we had a two-by-two table. It might even be a good route … except experience shows that the road has traps and pitfalls.\n\nNow I’m not disputing the mathematics that a likelihood is a kind of conditional probability. But “likelihood” is not a synonym for “conditional probability.” Our pedagogy ought to make clear what kinds of conditional probabilities are likelihoods.\nI take a different route. A minor part of this is to bypass the normalization in Bayes rule by saying that p() is a relative probability. We can do all the calculations without normalizing, waiting until we need to present the results as (non-relative) probabilities at the time we present the results to our client.\nThe major feature of my different route has to do with making certain and obvious that A and B are different kinds of things. One of the kinds of things is an outcome of an event, for instance, the event of taking a clinical test, which produces + or - result where some randomness may be involved.\nThe other kind of thing is a hypothesis. The word “hypothesis” is not a stranger to traditional Stat 101 where it appears in two guises: the Null hypothesis and the Alternative hypothesis.\nBut I introduce hypothesis more generally, in the sense used in the scientific method and philosophy. A hypothesis is merely a statement that might or might not be true in the real world. For example, “Bill is sick” is a hypothesis, as is “Bill is healthy.” A simulation (Lesson 14) is a hypothesis.\nTo give students a more concrete image of a hypothesis, I analogize them to planets. Imagine that you can create a planet, perhaps by simulation, in which things happen according to an exactly known process. You could also create a different planet, in which things happen by another process, which is also completely known. These planets are like hypotheses; you get to say what is going on (the statement) but it isn’t necessarily even close to what’s going on here on Earth (the statement might or might not be true).\nSo think of the B in Bayes rule as indicating which planet you are on.\nThe A in Bayes rule is not a hypothesis. Instead, it is an observation. For instance, “Bill had a positive test result” is a possible observation, as is “Bill had a negative test result.” Since A has already been observed, there’s no reason to worry about the probability of A.\nNow for likelihood, in concrete terms. Imagine you are on Planet B_sick_, the planet inhabited only by sick people. If you performed the clinical test on these sick people, a given individual might have a positive or a negative test result. The probability of a positive result, given that you are on Planet B_sick_ is what we call a likelihood. You calculate that likelihood knowing absolutely that you are, for the purposes of the calculation, on Planet B_sick_. (In detection theory, the probability of a + test result from an inhabitant of Planet B_sick_ is called the sensitivity of the test. Sensitivity is a likelihood.)\nOf course, you might decide to go to Planet B_healthy_, where all the inhabitants are healthy. On this planet, a negative test result is what’s expected, but the test is imperfect so we refer to the probability of a negative test (on Planet B_healthy_). This probability is a likelihood.\nA p-value is a likelihood. The planet involved is Planet Null, the planet where all model coefficients would be seen as zero, at least in the limit where an infinite amount of data is collected. You have an observation (from Planet Earth) of a model coefficient. The p-value is the likelihood of that Earthy observation (or more extreme) if you had conducted your work on Planet Null.\nNow for Bayes rule. Imagine you are on a space ship and for some reason you are interested in whether the person named Ella is healthy or sick. That is, you want to know whether Ella lives on Planet B_sick_ or Planet B_healthy_. You receive a radio message: Ella had a positive test result. Regretably, although you now know the test result for Ella, you’re not sure whether the message came from Planet B_sick_ or Planet B_healthy_. Your radio detection finder points vaguely to Planet B_healthy_ but Planet B_sick_ is in the same general direction. So it might be either. You assess that the probability is twice as high that the origin is Planet B_sick_ than Planet B_healthy_.\nTo calculate the update the probabilities indicated by your radio direction finder with the received information that Ella’s test result was positive, you do a specific calculation that involves two likelihoods: the likelihood of a + test result on Planet B_sick_ and the likelihood of a - test result on Planet B_healthy_. The higher the likelihood, the greater the probability that Ella lives on that planet. But you also have information from your radio direction finder. To make an overall judgement of whether Ella is more likely to be on Planet B_sick_ or Planet B_healthy_, you should combine the two sources of information. You can do by undertaking two different calculations of relative probability, the results of which can be compared to generate your conclusion.\n\nthe relative probability that Ella is sick. This is \\[\\underbrace{p(+\\ \\text{test result}\\ given\\ \\text{Ella lives on Planet Sick})}_\\text{A likelihood calculated on Planet Sick}\\times \\underbrace{p(\\text{Message came from Planet Sick})}_\\text{Based on the radio direction finder}\\]\nthe relative probability that Ella is healthy. This is very similar: \\[\\underbrace{p(+\\ \\text{test result}\\ given\\ \\text{Ella lives on Planet Healthy})}_\\text{A likelihood calculated on Planet Healthy}\\times \\underbrace{p(\\text{Message came from Planet Healthy})}_\\text{Based on the radio direction finder}\\] Dividing (i) by (ii) gives the “odds” of Ella living on Planet B_sick_. Odds are easily converted into probabilities, if that’s what you want. Actually, Bayes rule has a particularly nice form when there are only two hypotheses involved.\n\n\\[\\underbrace{Odds(\\text{Sick} | + )}_\\text{posterior odds} = \\underbrace{\\frac{Likelihood(+ | \\text{Sick})}{Likelihood(+ | \\text{Healthy})}}_\\text{Likelihood ratio} \\times \\underbrace{Odds(\\text{Sick} | \\text{radio direction finder})}_\\text{prior odds}\\]\nA likelihood is merely the probability of a given observation given that you are on a particular hypothetical planet."
  },
  {
    "objectID": "posts/Streamlining-graphics/index.html",
    "href": "posts/Streamlining-graphics/index.html",
    "title": "*Instructor’s Note: Streamlining graphics",
    "section": "",
    "text": "Lessons in Statistical Thinking incorporates only one form of data graphics, the “annotated point plot.” This bucks tradition, which calls for presenting a repertoire of several graphics types. So, there are no bar plots, very few probability density curves, and no box-and-whisker plots.\nNaturally, an instructor may choose to cover such diverse types of graphics, but they are not needed to follow the flow of the Lessons.\nWhy might an instructor do better by avoiding needless variety in graphical modes? Many traditional modes are obsolete, for example stem-and-leaf plots which were invented by Tufte to facilitate graphing distributions on a **typewriter*. Very few of our students have ever seen a typewriter, let alone used one. And many traditional modes have superior modern alternatives. Box-and-whisker plots are not as telling as violins. Similarly, continuous estimates of probability density (as in a violin) are superior to histograms in almost every way.\nOne of the chief aims in writing Lessons is to help shift the emphasis in college-level statistics from the distribution of a single variable to descriptions and analysis of the relationship between variables. The natural frame for a data graphic about relationships (and models of those relationships) involves mapping each axis to a variable.\nRegression models always involve a response variable. Graphics in Lessons always places that variable on the vertical axis. The first explanatory variable is always placed on the horizontal axis. (Color and faceting are used for a second or third explanatory variable.) Traditional introductory statistics emphasizes a single variable and the accompanying graphics sometimes place this on the horizontal and sometimes on the vertical axis. (See these examples.) Better to standardize."
  },
  {
    "objectID": "posts/Renovations/index.html",
    "href": "posts/Renovations/index.html",
    "title": "Innovation and renovation for intro stats",
    "section": "",
    "text": "George Cobb (1947-2020), an esteemed statistical educator, had a keen eye for metaphor. In a provocative article in The American Statistician, George’s central metaphor is a house that has become ill-suited to its residents. He argued, “Mere renovation is too little too late; we need to rethink our undergraduate curriculum from the ground up.”\nThe textbook Lessons in Statistical Thinking (LST) presents my ground-up rebuild of the introductory statistics course. This post is intended for instructors who are starting the move from the old house to the new, to prepare them for many of the innovations they will encounter."
  },
  {
    "objectID": "posts/Renovations/index.html#teach-data-organization",
    "href": "posts/Renovations/index.html#teach-data-organization",
    "title": "Innovation and renovation for intro stats",
    "section": "Teach data organization",
    "text": "Teach data organization"
  },
  {
    "objectID": "posts/Renovations/index.html#simplify-and-unify-graphics",
    "href": "posts/Renovations/index.html#simplify-and-unify-graphics",
    "title": "Innovation and renovation for intro stats",
    "section": "Simplify and unify graphics",
    "text": "Simplify and unify graphics"
  },
  {
    "objectID": "posts/Renovations/index.html#computational-patterns",
    "href": "posts/Renovations/index.html#computational-patterns",
    "title": "Innovation and renovation for intro stats",
    "section": "Computational patterns",
    "text": "Computational patterns"
  },
  {
    "objectID": "posts/Renovations/index.html#variation-and-variance",
    "href": "posts/Renovations/index.html#variation-and-variance",
    "title": "Innovation and renovation for intro stats",
    "section": "Variation and variance",
    "text": "Variation and variance"
  },
  {
    "objectID": "posts/Renovations/index.html#renovated-vocabulary",
    "href": "posts/Renovations/index.html#renovated-vocabulary",
    "title": "Innovation and renovation for intro stats",
    "section": "Renovated vocabulary",
    "text": "Renovated vocabulary\nAs a rule, it’s good to respect the accepted vocabulary of a field. But much of the vocabulary used in introductory statistics introduces confusion to students\nCoefficient: Correlation coefficient vs model coefficients. R2 (“R-squared”) rather than “coefficient of determination.”\nStandard deviation: Switch to “variance,” which is a built-in reminder of exactly what it measure: variation.\nStandard error:\nData set, table, data table:\nUse “data frame” instead. In LST, data frames always satisfy the principles of “tidy” data organization. In contrast, a “table” is a summary of data for the human reader, which can be organized in whatever way best serves the purpose of communication with the reader.\nData frames are covered in the first Lesson of LST because they establish essential vocabulary needed in all later Lessons.\nRow, case, data point, unit:\nThese are mainly used as synonyms. LST uses “specimen” to refer to what is being represented by a row in a data frame. Think of a museum collection, which contains many objects collected from the real world. Each of those objects is a “specimen.”"
  },
  {
    "objectID": "posts/LSTbook-on-CRAN/index.html",
    "href": "posts/LSTbook-on-CRAN/index.html",
    "title": "{LSTbook} R package released on CRAN",
    "section": "",
    "text": "After more than a year of development, the {LSTbook} R package has been released to CRAN, the official distribution site for R packages. You can browse through the package vignettes and documentation at package repository and see how the package is used in the book Lessons in Statistical Thinking."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Instructor Blog for *Lessons in Statistical Thinking*",
    "section": "",
    "text": "Nonlinear Modeling: Something Fishy\n\n\n\n\n\n\nregression\n\n\nlogarithms\n\n\nallometry\n\n\ncomputing\n\n\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nDaniel Kaplan based on Douglas Whitaker’s STEW Lesson Plan\n\n\n\n\n\n\n\n\n\n\n\n\nLST computing in your browser\n\n\n\n\n\n\nLSTbook\n\n\ncomputing\n\n\n\n\n\n\n\n\n\nApr 3, 2024\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation and causation\n\n\n\n\n\n\ncorrelation\n\n\ncausality\n\n\n\n\n\n\n\n\n\nMar 31, 2024\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\n\n\n\n\nReducing R friction with the {LSTbook} package\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 29, 2024\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\n\n\n\n\nReforming Stat 101 with Lessons in Statistical Thinking\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\n\n\n\n\nPythagorean statistics\n\n\n\n\n\n\nresiduals\n\n\nvariance\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\n\n\n\n\n*Instructor’s Note: Likelihood\n\n\n\n\n\n\nlikelihood\n\n\ninstructor note\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\n\n\n\n\n*Instructor’s Note: Streamlining graphics\n\n\n\n\n\n\ngraphics\n\n\ninstructor note\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\n\n\n\n\nInnovation and renovation for intro stats\n\n\n\n\n\n\nsoftware\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\n\n\n\n\n{LSTbook} R package released on CRAN\n\n\n\n\n\n\nsoftware\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\n\n\n\n\nA Model Statistics Course\n\n\n\n\n\n\npresentation\n\n\noverview\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\n\n\n\n\nLesson 12 Resources\n\n\n\n\n\n\nLesson 12\n\n\ncovariates\n\n\nadjustment\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nDaniel Kaplan\n\n\n\n\n\n\n\n\n\n\n\n\nLesson 23 Resources\n\n\n\n\n\n\nLesson 23\n\n\nconfidence intervals\n\n\ncovariates\n\n\nadjustment\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nDaniel Kaplan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Trout-perch-allometrics/index.html#initial-analysis-simple-linear-regression",
    "href": "posts/Trout-perch-allometrics/index.html#initial-analysis-simple-linear-regression",
    "title": "Nonlinear Modeling: Something Fishy",
    "section": "Initial Analysis (Simple Linear Regression)",
    "text": "Initial Analysis (Simple Linear Regression)\nExamine the relationship Weight versus Length in the data, comparing it to a straight-line model. The point_plot() function will show the data and the model.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIt can be a little difficult to see the model; it’s obscured in some places by the data points. Add these two arguments to point_plot() to make the model easier to see: point_ink = 0.1, model_ink = 1.\nLet’s fit the linear model so that we can look at the coefficients and R2.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWould you describe the R2 as high, medium, or low?\nWhat is the meaning of the Length coefficient (0.1766) in terms of how fish weight is associated with length`?\nThe confidence interval on the Length coefficient is very tight. Is this consistent with the R2 value?\n\nThe numbers from the linear model are promising. But it pays to look closer. Referring to the point plot, are there any systematic deviations of the model from the data? For instance, do the data show a systematic curved shape?\n\nAt short Length, does the model tend to overestimate or underestimate the Weight?\nAt long Length, does the model tend to overestimate or underestimate the Weight?\nHow about at middle Length.\n\nA model that systematically deviates from the data calls for improvement! How should we do so?\nThere is a fascinating field called allometry that looks at the shape of bodies or body parts as a function of size. For example, an infant has a body shape that is not just a scaled-down version of an adult. Infants have large heads and short legs and arms compared to adults.\nOne of the general finding of allometry is that body measurements do not grow linearly. Instead, the growth is proportionate. This means that a given percentage increase in length corresponds to a given percentage increase in weight. This idea of proportionality is captured by the mathematical idea of logarithms.\nYou don’t have to worry about remembering the logarithm formulas you learned in high-school. The computer happily will deal with the logs, and we’ll show how to interpret them. All you have to do is take the log() of each of the variables.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nJudging from the graph, is the log-log model a better match to the data?\n\nLooking the model quantitatively:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe model formula corresponding to these coefficients is\n\\[\\log(weight) = -4.9 + 2.97 \\log(length) .\\]\nThis is where those high-school formulas about logarithms come in handy. In particular, this one: \\(weight = 10^{log(weight)}\\). This means that we can translate the formula into something without logs in sight.\n\\[weight = (10^{-4.9})\\ length^{2.97} = 0.0000126\\ length^{2.97}\\]\n\nLook at the original (non-logarithmic) data, what’s a typical weight for an 80mm long fish?\nNow plug 80mm into the formula for weight.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nDoes the result match what you found in (8)?\nNow redo the calculation plugging a length of 100mm into the formula. How does the result compare to the data for fish that are 100mm long?"
  }
]